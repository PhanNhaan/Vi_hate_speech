{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-01 20:02:05.070444: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-01 20:02:05.417580: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-01 20:02:06.597459: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-06-01 20:02:06.597645: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-06-01 20:02:06.597653: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Input, Reshape, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, BatchNormalization, Concatenate\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Dropout, ReLU\n",
    "from tensorflow.keras.layers import Dense, Concatenate, Bidirectional, GRU, Conv1D, GlobalAveragePooling1D, GlobalMaxPooling1D, SpatialDropout1D, Dropout, concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-01 20:02:08.546610: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-01 20:02:08.674857: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-01 20:02:08.674921: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU'))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# project_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)), '..')\n",
    "project_dir = os.path.abspath(os.path.join(os.path.dirname(os.path.realpath('__file__')), '..'))\n",
    "data_dir = os.path.join(project_dir, 'preprocessing')\n",
    "sys.path.append(data_dir)\n",
    "\n",
    "from preprocess import preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(y_pred, y_test):\n",
    "    # Tính toán các chỉ số đánh giá\n",
    "    accuracy  = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1_micro = f1_score(y_test, y_pred, average='micro')\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1-micro: {f1_micro}\")\n",
    "    print(f\"F1-macro: {f1_macro}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_svd(X_train, X_test, X_dev):\n",
    "    tfidf_vect = TfidfVectorizer(analyzer='word', max_features=30000)\n",
    "    tfidf_vect.fit(X_train) # learn vocabulary and idf from training set\n",
    "    X_train_tfidf =  tfidf_vect.transform(X_train)\n",
    "    X_test_tfidf =  tfidf_vect.transform(X_test)\n",
    "    X_dev_tfidf =  tfidf_vect.transform(X_dev)\n",
    "\n",
    "    svd = TruncatedSVD(n_components=300, random_state=42)\n",
    "    svd.fit(X_train_tfidf)\n",
    "\n",
    "    X_train_tfidf_svd = svd.transform(X_train_tfidf)\n",
    "    X_test_tfidf_svd = svd.transform(X_test_tfidf)\n",
    "    X_dev_tfidf_svd = svd.transform(X_dev_tfidf)\n",
    "\n",
    "    return X_train_tfidf_svd, X_test_tfidf_svd, X_dev_tfidf_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec(X_train, X_test, X_dev):\n",
    "    tokenizer = Tokenizer(num_words=10000)\n",
    "    tokenizer.fit_on_texts(X_train) \n",
    "                                \n",
    "    sequences = tokenizer.texts_to_sequences(X_train)\n",
    "    X_train_feature = pad_sequences(sequences, maxlen=300) \n",
    "    # tr_y = to_categorical(tr_label)\n",
    "\n",
    "    sequences = tokenizer.texts_to_sequences(X_test)\n",
    "    X_test_feature = pad_sequences(sequences, maxlen=300)\n",
    "    # val_y = to_categorical(val_label)\n",
    "\n",
    "    sequences = tokenizer.texts_to_sequences(X_dev)\n",
    "    X_dev_feature = pad_sequences(sequences, maxlen=300)\n",
    "    # ts_y = to_categorical(ts_label)\n",
    "\n",
    "    return X_train_feature, X_test_feature, X_dev_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1\n",
    "def create_lstm_model():\n",
    "    input_layer = Input(shape=(300,))\n",
    "    emb = Embedding(10000, 200, input_length=300, trainable=True)(input_layer)\n",
    "    \n",
    "    # layer = Reshape((10, 30))(input_layer)\n",
    "    layer = Sequential()(emb)\n",
    "    layer = LSTM(128, activation='relu', return_sequences=True)(layer)\n",
    "    layer = Dense(512, activation='relu')(layer)\n",
    "    layer = Dense(512, activation='relu')(layer)\n",
    "    layer = Dense(128, activation='relu')(layer)\n",
    "    \n",
    "    output_layer = Dense(10, activation='softmax')(layer)\n",
    "    \n",
    "    classifier = models.Model(input_layer, output_layer)\n",
    "    \n",
    "    classifier.compile(optimizer=optimizers.legacy.Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "def create_lstm_model():\n",
    "    input = Input(shape = (300,))\n",
    "    emb = Embedding(10000, 200, input_length=300, trainable=True)(input)\n",
    "    # emb = Reshape((10, 30))(input)\n",
    "    x1 = SpatialDropout1D(0.2)(emb)\n",
    "\n",
    "    x = Bidirectional(GRU(112, return_sequences = True))(x1)\n",
    "    x = Conv1D(3, kernel_size = 3, padding = \"valid\", kernel_initializer = \"he_uniform\")(x)\n",
    "        \n",
    "    y = Bidirectional(LSTM(112, return_sequences = True))(x1)\n",
    "    y = Conv1D(3, kernel_size = 3, padding = \"valid\", kernel_initializer = \"he_uniform\")(y)\n",
    "        \n",
    "    avg_pool1 = GlobalAveragePooling1D()(x)\n",
    "    max_pool1 = GlobalMaxPooling1D()(x)\n",
    "        \n",
    "    avg_pool2 = GlobalAveragePooling1D()(y)\n",
    "    max_pool2 = GlobalMaxPooling1D()(y)\n",
    "\n",
    "    conc = Concatenate(axis=-1)([avg_pool1, max_pool1, avg_pool2, max_pool2])\n",
    "\n",
    "    hid_layer = Dense(128, activation='relu')(conc)\n",
    "    dropout = Dropout(0.3)(hid_layer)\n",
    "    output_layer = Dense(10, activation='softmax')(dropout)\n",
    "    \n",
    "    classifier = models.Model(inputs=input, outputs=output_layer)\n",
    "    \n",
    "    classifier.compile(optimizer=optimizers.legacy.Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "def create_lstm_model():\n",
    "    input = Input(shape=(300,))\n",
    "    emb = Embedding(10000, 200, input_length=300, trainable=True)(input)\n",
    "    # emb = Reshape((10, 30))(input)\n",
    "\n",
    "    branch1 = Sequential()(emb)\n",
    "    branch1=LSTM(64, return_sequences=True)(branch1)\n",
    "    branch1=BatchNormalization()(branch1)\n",
    "    branch1=ReLU()(branch1)\n",
    "    branch1=Dropout(0.5)(branch1)\n",
    "    branch1=GlobalMaxPooling1D()(branch1)\n",
    "    \n",
    "        # Branch 2\n",
    "    branch2 = Sequential()(emb)\n",
    "    branch2=LSTM(64, return_sequences=True)(branch2)\n",
    "    branch2=BatchNormalization()(branch2)\n",
    "    branch2=ReLU()(branch2)\n",
    "    branch2=Dropout(0.5)(branch2)\n",
    "    branch2=GlobalMaxPooling1D()(branch2)\n",
    "\n",
    "    concatenated = Concatenate()([branch1, branch2])\n",
    "\n",
    "    hid_layer = Dense(128, activation='relu')(concatenated)\n",
    "    dropout = Dropout(0.3)(hid_layer)\n",
    "    output_layer = Dense(10, activation='softmax')(dropout)\n",
    "    \n",
    "    classifier = models.Model(inputs=input, outputs=output_layer)\n",
    "    \n",
    "    classifier.compile(optimizer=optimizers.legacy.Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ViHSD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/ViHSD/train.csv\")\n",
    "test = pd.read_csv(\"../data/ViHSD/test.csv\")\n",
    "dev = pd.read_csv(\"../data/ViHSD/dev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label_id\n",
       "0    19886\n",
       "1     4162\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['label_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[\"free_text\"].copy()\n",
    "y_train = train[\"label_id\"].copy()\n",
    "\n",
    "X_test = test[\"free_text\"].copy()\n",
    "y_test = test[\"label_id\"].copy()\n",
    "\n",
    "X_dev = dev[\"free_text\"].copy()\n",
    "y_dev = dev[\"label_id\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.apply(preprocess)\n",
    "X_test = X_test.apply(preprocess)\n",
    "X_dev = X_dev.apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_feature, X_test_feature, X_dev_feature = tfidf_svd(X_train, X_test, X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_feature, X_test_feature, X_dev_feature = word2vec(X_train, X_test, X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "94/94 [==============================] - 21s 174ms/step - loss: 1.2781 - accuracy: 0.7032 - val_loss: 0.3821 - val_accuracy: 0.8432\n",
      "Epoch 2/100\n",
      "94/94 [==============================] - 15s 157ms/step - loss: 0.5399 - accuracy: 0.8635 - val_loss: 0.3285 - val_accuracy: 0.8660\n",
      "Epoch 3/100\n",
      "94/94 [==============================] - 14s 146ms/step - loss: 0.4112 - accuracy: 0.8946 - val_loss: 0.3208 - val_accuracy: 0.8690\n",
      "Epoch 4/100\n",
      "94/94 [==============================] - 13s 141ms/step - loss: 0.3369 - accuracy: 0.9127 - val_loss: 0.4043 - val_accuracy: 0.8361\n",
      "Epoch 5/100\n",
      "94/94 [==============================] - 14s 145ms/step - loss: 0.2732 - accuracy: 0.9315 - val_loss: 0.4166 - val_accuracy: 0.8406\n",
      "Epoch 6/100\n",
      "94/94 [==============================] - 13s 141ms/step - loss: 0.2229 - accuracy: 0.9447 - val_loss: 0.4869 - val_accuracy: 0.8278\n",
      "Epoch 7/100\n",
      "94/94 [==============================] - 14s 145ms/step - loss: 0.1933 - accuracy: 0.9531 - val_loss: 0.4623 - val_accuracy: 0.8548\n",
      "Epoch 8/100\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1644 - accuracy: 0.9615Restoring model weights from the end of the best epoch: 3.\n",
      "94/94 [==============================] - 13s 142ms/step - loss: 0.1644 - accuracy: 0.9615 - val_loss: 0.5082 - val_accuracy: 0.8660\n",
      "Epoch 8: early stopping\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5, restore_best_weights=True,min_delta=0.001)\n",
    "class_weights = {0: 1.0, 1: 4.0}\n",
    "\n",
    "model_hsd = create_lstm_model()\n",
    "history = model_hsd.fit(X_train_feature, y_train, \n",
    "                    validation_data=(X_dev_feature, y_dev), \n",
    "                    batch_size=256, epochs=100, verbose=True,\n",
    "                    callbacks=[early_stopping],\n",
    "                    class_weight=class_weights\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_hsd.save('../models/model_lstm_hsd_2_w2v.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209/209 [==============================] - 10s 41ms/step\n",
      "Accuracy: 0.8670658682634731\n",
      "Precision: 0.5893118594436311\n",
      "Recall: 0.7111307420494699\n",
      "F1-micro: 0.8670658682634731\n",
      "F1-macro: 0.7813813562357896\n"
     ]
    }
   ],
   "source": [
    "model_eva = create_lstm_model()\n",
    "model_eva.load_weights('../models/model_lstm_hsd_2_w2v.h5')\n",
    "# model_eva= model_hsd\n",
    "\n",
    "y_pred = model_eva.predict(X_test_feature)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "evaluation(y_pred_classes, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 - micro: 0.8670658682634731\n",
      "F1 - macro: 0.7813813562357896\n",
      "Accuracy: 0.8670658682634731\n",
      "Precision - macro: 0.7638881465076642\n",
      "Recall - macro: 0.8050066111112526\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "# Tính toán các chỉ số đánh giá\n",
    "f1_micro = f1_score(y_test, y_pred_classes, average='micro')\n",
    "print(\"F1 - micro: \" + str(f1_micro))\n",
    "\n",
    "f1_macro = f1_score(y_test, y_pred_classes, average='macro')\n",
    "print(\"F1 - macro: \" + str(f1_macro))\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "print(\"Accuracy: \" + str(accuracy))\n",
    "\n",
    "precision_macro = precision_score(y_test, y_pred_classes, average='macro')\n",
    "print(\"Precision - macro: \" + str(precision_macro))\n",
    "\n",
    "recall_macro = recall_score(y_test, y_pred_classes, average='macro')\n",
    "print(\"Recall - macro: \" + str(recall_macro))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 5314, 1: 1366})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "element_counts = Counter(y_pred_classes)\n",
    "print(element_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "\n",
    "# w2v\n",
    "\n",
    "# Accuracy: 0.8670658682634731\n",
    "# Precision: 0.5893118594436311\n",
    "# Recall: 0.7111307420494699\n",
    "# F1-micro: 0.8670658682634731\n",
    "# F1-macro: 0.7813813562357896\n",
    "\n",
    "# F1 - micro: 0.8670658682634731\n",
    "# F1 - macro: 0.7813813562357896\n",
    "# Accuracy: 0.8670658682634731\n",
    "# Precision - macro: 0.7638881465076642\n",
    "# Recall - macro: 0.8050066111112526"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 3\n",
    "\n",
    "# Accuracy: 0.8591317365269461\n",
    "# Precision: 0.5920925747348119\n",
    "# Recall: 0.5424028268551236\n",
    "# F1-micro: 0.8591317365269461\n",
    "# F1-macro: 0.7410370473638934\n",
    "\n",
    "# w2v\n",
    "\n",
    "# Accuracy: 0.8565868263473054\n",
    "# Precision: 0.5663109756097561\n",
    "# Recall: 0.6563604240282686\n",
    "# F1-micro: 0.8565868263473054\n",
    "# F1-macro: 0.7601292776440715"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 1\n",
    "\n",
    "# tf-idf\n",
    "# Accuracy: 0.8494011976047904\n",
    "# Precision: 0.5474397590361446\n",
    "# Recall: 0.642226148409894\n",
    "# F1-micro: 0.8494011976047903\n",
    "# F1-macro: 0.7493816662937272"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CTSD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/ViCTSD/ViCTSD_train.csv\")\n",
    "test = pd.read_csv(\"../data/ViCTSD/ViCTSD_test.csv\")\n",
    "dev = pd.read_csv(\"../data/ViCTSD/ViCTSD_valid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lọc các hàng có nhãn 1\n",
    "# df_label_1 = train[train['Toxicity'] == 1]\n",
    "\n",
    "# # Nhân đôi các hàng có nhãn 1\n",
    "# df_label_1_doubled = pd.concat([df_label_1] * 3, ignore_index=True)\n",
    "\n",
    "# # Kết hợp với DataFrame gốc\n",
    "# train = pd.concat([train, df_label_1_doubled], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Toxicity\n",
       "0    6241\n",
       "1    3036\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Toxicity\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lọc các hàng có nhãn 1\n",
    "# df_label_1 = dev[dev['Toxicity'] == 1]\n",
    "\n",
    "# # Nhân đôi các hàng có nhãn 1\n",
    "# df_label_1_doubled = pd.concat([df_label_1] * 3, ignore_index=True)\n",
    "\n",
    "# # Kết hợp với DataFrame gốc\n",
    "# dev = pd.concat([dev, df_label_1_doubled], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Toxicity\n",
       "0    1768\n",
       "1     928\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev[\"Toxicity\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[\"Comment\"].copy()\n",
    "y_train = train[\"Toxicity\"].copy()\n",
    "\n",
    "X_test = test[\"Comment\"].copy()\n",
    "y_test = test[\"Toxicity\"].copy()\n",
    "\n",
    "X_dev = dev[\"Comment\"].copy()\n",
    "y_dev = dev[\"Toxicity\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.apply(preprocess)\n",
    "X_test = X_test.apply(preprocess)\n",
    "X_dev = X_dev.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def augment_data(X, y, num_augmented_samples=10000):\n",
    "#     augmented_X = []\n",
    "#     augmented_y = []\n",
    "#     for _ in range(num_augmented_samples):\n",
    "#         idx = np.random.choice(len(X))\n",
    "#         sample = X[idx]\n",
    "#         label = y[idx]\n",
    "#         # Thực hiện một số phép biến đổi ngẫu nhiên, ví dụ thay đổi một số từ trong mẫu\n",
    "#         augmented_sample = sample.copy()\n",
    "#         change_idx = np.random.choice(len(sample), size=5, replace=False)\n",
    "#         augmented_sample[change_idx] = np.random.randint(1, 5000, size=5)\n",
    "#         augmented_X.append(augmented_sample)\n",
    "#         augmented_y.append(label)\n",
    "#     return np.array(augmented_X), np.array(augmented_y)\n",
    "# augmented_X, augmented_y = augment_data(X_train_feature, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_feature, X_test_feature, X_dev_feature = tfidf_svd(X_train, X_test, X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_feature, X_test_feature, X_dev_feature = word2vec(X_train, X_test, X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_cate = to_categorical(y_train)\n",
    "# y_test_cate = to_categorical(y_test)\n",
    "# y_dev_cate = to_categorical(y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "28/28 [==============================] - 6s 102ms/step - loss: 1.1338 - accuracy: 0.6929 - val_loss: 2.1394 - val_accuracy: 0.8840\n",
      "Epoch 2/100\n",
      "28/28 [==============================] - 2s 80ms/step - loss: 0.6415 - accuracy: 0.8449 - val_loss: 2.0651 - val_accuracy: 0.8925\n",
      "Epoch 3/100\n",
      "28/28 [==============================] - 2s 82ms/step - loss: 0.4347 - accuracy: 0.8950 - val_loss: 1.9921 - val_accuracy: 0.8840\n",
      "Epoch 4/100\n",
      "28/28 [==============================] - 2s 75ms/step - loss: 0.2783 - accuracy: 0.9353 - val_loss: 1.8875 - val_accuracy: 0.8810\n",
      "Epoch 5/100\n",
      "28/28 [==============================] - 2s 75ms/step - loss: 0.2089 - accuracy: 0.9533 - val_loss: 1.7837 - val_accuracy: 0.8705\n",
      "Epoch 6/100\n",
      "28/28 [==============================] - 2s 76ms/step - loss: 0.1490 - accuracy: 0.9639 - val_loss: 1.6695 - val_accuracy: 0.8590\n",
      "Epoch 7/100\n",
      "28/28 [==============================] - 2s 74ms/step - loss: 0.1275 - accuracy: 0.9726 - val_loss: 1.5817 - val_accuracy: 0.8295\n",
      "Epoch 8/100\n",
      "28/28 [==============================] - 2s 77ms/step - loss: 0.1027 - accuracy: 0.9756 - val_loss: 1.4228 - val_accuracy: 0.8350\n",
      "Epoch 9/100\n",
      "28/28 [==============================] - 2s 73ms/step - loss: 0.0885 - accuracy: 0.9804 - val_loss: 1.2948 - val_accuracy: 0.8445\n",
      "Epoch 10/100\n",
      "28/28 [==============================] - 2s 75ms/step - loss: 0.0678 - accuracy: 0.9857 - val_loss: 1.0435 - val_accuracy: 0.8740\n",
      "Epoch 11/100\n",
      "28/28 [==============================] - 2s 75ms/step - loss: 0.0600 - accuracy: 0.9873 - val_loss: 0.9400 - val_accuracy: 0.8550\n",
      "Epoch 12/100\n",
      "28/28 [==============================] - 2s 76ms/step - loss: 0.0548 - accuracy: 0.9883 - val_loss: 0.8271 - val_accuracy: 0.8465\n",
      "Epoch 13/100\n",
      "28/28 [==============================] - 2s 78ms/step - loss: 0.0491 - accuracy: 0.9894 - val_loss: 0.7053 - val_accuracy: 0.8610\n",
      "Epoch 14/100\n",
      "28/28 [==============================] - 2s 76ms/step - loss: 0.0492 - accuracy: 0.9894 - val_loss: 0.5783 - val_accuracy: 0.8695\n",
      "Epoch 15/100\n",
      "28/28 [==============================] - 2s 77ms/step - loss: 0.0572 - accuracy: 0.9863 - val_loss: 0.5363 - val_accuracy: 0.8565\n",
      "Epoch 16/100\n",
      "28/28 [==============================] - 2s 78ms/step - loss: 0.0482 - accuracy: 0.9894 - val_loss: 0.5393 - val_accuracy: 0.8245\n",
      "Epoch 17/100\n",
      "28/28 [==============================] - 2s 78ms/step - loss: 0.0441 - accuracy: 0.9910 - val_loss: 0.4225 - val_accuracy: 0.8700\n",
      "Epoch 18/100\n",
      "28/28 [==============================] - 2s 77ms/step - loss: 0.0473 - accuracy: 0.9909 - val_loss: 0.4398 - val_accuracy: 0.8665\n",
      "Epoch 19/100\n",
      "28/28 [==============================] - 2s 77ms/step - loss: 0.0546 - accuracy: 0.9860 - val_loss: 0.4702 - val_accuracy: 0.8455\n",
      "Epoch 20/100\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.0327 - accuracy: 0.9923Restoring model weights from the end of the best epoch: 17.\n",
      "28/28 [==============================] - 2s 80ms/step - loss: 0.0324 - accuracy: 0.9924 - val_loss: 0.4834 - val_accuracy: 0.8515\n",
      "Epoch 20: early stopping\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3, restore_best_weights=True,min_delta=0.001)\n",
    "class_weights = {0: 1.0, 1: 5.0}\n",
    "model_ctsd = create_lstm_model()\n",
    "with tf.device('/gpu:0'):\n",
    "    history = model_ctsd.fit(X_train_feature, y_train, \n",
    "                        validation_data=(X_dev_feature, y_dev),\n",
    "                        epochs=100, batch_size=256, verbose=True,\n",
    "                        callbacks=[early_stopping],\n",
    "                        class_weight=class_weights\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ctsd.save('../models/model_lstm_ctsd_3_w2v.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 2s 42ms/step\n",
      "Accuracy: 0.869\n",
      "Precision: 0.4198473282442748\n",
      "Recall: 0.5\n",
      "F1-micro: 0.869\n",
      "F1-macro: 0.6909787011197894\n"
     ]
    }
   ],
   "source": [
    "model_eva = create_lstm_model()\n",
    "model_eva.load_weights('../models/model_lstm_ctsd_2_w2v.h5')\n",
    "# model_eva= model_ctsd\n",
    "\n",
    "y_pred = model_eva.predict(X_test_feature)\n",
    "y_pred_classes = np.argmax(y_pred, axis=-1)\n",
    "\n",
    "evaluation(y_pred_classes, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 - micro: 0.869\n",
      "F1 - macro: 0.6909787011197894\n",
      "Accuracy: 0.869\n",
      "Precision - macro: 0.6782780945018843\n",
      "Recall - macro: 0.7073033707865168\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "# Tính toán các chỉ số đánh giá\n",
    "f1_micro = f1_score(y_test, y_pred_classes, average='micro')\n",
    "print(\"F1 - micro: \" + str(f1_micro))\n",
    "\n",
    "f1_macro = f1_score(y_test, y_pred_classes, average='macro')\n",
    "print(\"F1 - macro: \" + str(f1_macro))\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "print(\"Accuracy: \" + str(accuracy))\n",
    "\n",
    "precision_macro = precision_score(y_test, y_pred_classes, average='macro')\n",
    "print(\"Precision - macro: \" + str(precision_macro))\n",
    "\n",
    "recall_macro = recall_score(y_test, y_pred_classes, average='macro')\n",
    "print(\"Recall - macro: \" + str(recall_macro))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 890, 1: 110})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "element_counts = Counter(y_test)\n",
    "print(element_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 869, 1: 131})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "element_counts = Counter(y_pred_classes)\n",
    "print(element_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "\n",
    "# Accuracy: 0.879\n",
    "# Precision: 0.39622641509433965\n",
    "# Recall: 0.19090909090909092\n",
    "# F1-micro: 0.879\n",
    "# F1-macro: 0.5959002240916939\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2\n",
    "\n",
    "# Accuracy: 0.869\n",
    "# Precision: 0.4198473282442748\n",
    "# Recall: 0.5\n",
    "# F1-micro: 0.869\n",
    "# F1-macro: 0.6909787011197894\n",
    "\n",
    "# F1 - micro: 0.869\n",
    "# F1 - macro: 0.6909787011197894\n",
    "# Accuracy: 0.869\n",
    "# Precision - macro: 0.6782780945018843\n",
    "# Recall - macro: 0.7073033707865168\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 3\n",
    "\n",
    "# Accuracy: 0.886\n",
    "# Precision: 0.47297297297297297\n",
    "# Recall: 0.3181818181818182\n",
    "# F1-micro: 0.886\n",
    "# F1-macro: 0.65882972610611"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
