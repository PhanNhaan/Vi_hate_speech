{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 23:07:32.044352: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-06 23:07:32.294531: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-06 23:07:33.283065: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-06-06 23:07:33.283771: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-06-06 23:07:33.283787: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Input, Reshape, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, BatchNormalization, Concatenate\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Dropout, ReLU\n",
    "from tensorflow.keras.layers import Dense, Concatenate, Bidirectional, GRU, Conv1D, GlobalAveragePooling1D, GlobalMaxPooling1D, SpatialDropout1D, Dropout, concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 23:07:34.902443: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-06 23:07:35.009115: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-06 23:07:35.009286: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU'))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# project_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)), '..')\n",
    "project_dir = os.path.abspath(os.path.join(os.path.dirname(os.path.realpath('__file__')), '..'))\n",
    "data_dir = os.path.join(project_dir, 'preprocessing')\n",
    "sys.path.append(data_dir)\n",
    "\n",
    "from preprocess import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phannhaan/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/phannhaan/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "# , AutoModelForSequenceClassification, Trainer, TrainingArguments, BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"vinai/phobert-base\", num_labels = 3)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\",use_fast=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(y_pred, y_test):\n",
    "    # T√≠nh to√°n c√°c ch·ªâ s·ªë ƒë√°nh gi√°\n",
    "    accuracy  = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1_micro = f1_score(y_test, y_pred, average='micro')\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1-micro: {f1_micro}\")\n",
    "    print(f\"F1-macro: {f1_macro}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_svd(X_train, X_test, X_dev):\n",
    "    tfidf_vect = TfidfVectorizer(analyzer='word', max_features=30000)\n",
    "    tfidf_vect.fit(X_train) # learn vocabulary and idf from training set\n",
    "    X_train_tfidf =  tfidf_vect.transform(X_train)\n",
    "    X_test_tfidf =  tfidf_vect.transform(X_test)\n",
    "    X_dev_tfidf =  tfidf_vect.transform(X_dev)\n",
    "\n",
    "    svd = TruncatedSVD(n_components=300, random_state=42)\n",
    "    svd.fit(X_train_tfidf)\n",
    "\n",
    "    X_train_tfidf_svd = svd.transform(X_train_tfidf)\n",
    "    X_test_tfidf_svd = svd.transform(X_test_tfidf)\n",
    "    X_dev_tfidf_svd = svd.transform(X_dev_tfidf)\n",
    "\n",
    "    return X_train_tfidf_svd, X_test_tfidf_svd, X_dev_tfidf_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec(X_train, X_test, X_dev):\n",
    "    tokenizer = Tokenizer(num_words=10000)\n",
    "    tokenizer.fit_on_texts(X_train) \n",
    "                                \n",
    "    sequences = tokenizer.texts_to_sequences(X_train)\n",
    "    X_train_feature = pad_sequences(sequences, maxlen=300) \n",
    "    # tr_y = to_categorical(tr_label)\n",
    "\n",
    "    sequences = tokenizer.texts_to_sequences(X_test)\n",
    "    X_test_feature = pad_sequences(sequences, maxlen=300)\n",
    "    # val_y = to_categorical(val_label)\n",
    "\n",
    "    sequences = tokenizer.texts_to_sequences(X_dev)\n",
    "    X_dev_feature = pad_sequences(sequences, maxlen=300)\n",
    "    # ts_y = to_categorical(ts_label)\n",
    "\n",
    "    return X_train_feature, X_test_feature, X_dev_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1\n",
    "def create_lstm_model():\n",
    "    input_layer = Input(shape=(300,))\n",
    "    emb = Embedding(10000, 200, input_length=300, trainable=True)(input_layer)\n",
    "    \n",
    "    # layer = Reshape((10, 30))(input_layer)\n",
    "    layer = Sequential()(emb)\n",
    "    layer = LSTM(128, activation='relu', return_sequences=True)(layer)\n",
    "    layer = Dense(512, activation='relu')(layer)\n",
    "    layer = Dense(512, activation='relu')(layer)\n",
    "    layer = Dense(128, activation='relu')(layer)\n",
    "    \n",
    "    output_layer = Dense(10, activation='softmax')(layer)\n",
    "    \n",
    "    classifier = models.Model(input_layer, output_layer)\n",
    "    \n",
    "    classifier.compile(optimizer=optimizers.legacy.Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "def create_lstm_model():\n",
    "    input = Input(shape = (300,))\n",
    "    emb = Embedding(10000, 200, input_length=300, trainable=True)(input)\n",
    "    # emb = Reshape((10, 30))(input)\n",
    "    x1 = SpatialDropout1D(0.2)(emb)\n",
    "\n",
    "    x = Bidirectional(GRU(112, return_sequences = True))(x1)\n",
    "    x = Conv1D(3, kernel_size = 3, padding = \"valid\", kernel_initializer = \"he_uniform\")(x)\n",
    "        \n",
    "    y = Bidirectional(LSTM(112, return_sequences = True))(x1)\n",
    "    y = Conv1D(3, kernel_size = 3, padding = \"valid\", kernel_initializer = \"he_uniform\")(y)\n",
    "        \n",
    "    avg_pool1 = GlobalAveragePooling1D()(x)\n",
    "    max_pool1 = GlobalMaxPooling1D()(x)\n",
    "        \n",
    "    avg_pool2 = GlobalAveragePooling1D()(y)\n",
    "    max_pool2 = GlobalMaxPooling1D()(y)\n",
    "\n",
    "    conc = Concatenate(axis=-1)([avg_pool1, max_pool1, avg_pool2, max_pool2])\n",
    "\n",
    "    hid_layer = Dense(128, activation='relu')(conc)\n",
    "    dropout = Dropout(0.3)(hid_layer)\n",
    "    output_layer = Dense(10, activation='softmax')(dropout)\n",
    "    \n",
    "    classifier = models.Model(inputs=input, outputs=output_layer)\n",
    "    \n",
    "    classifier.compile(optimizer=optimizers.legacy.Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "def create_lstm_model():\n",
    "    input = Input(shape=(300,))\n",
    "    emb = Embedding(10000, 200, input_length=300, trainable=True)(input)\n",
    "    # emb = Reshape((10, 30))(input)\n",
    "\n",
    "    branch1 = Sequential()(emb)\n",
    "    branch1=LSTM(64, return_sequences=True)(branch1)\n",
    "    branch1=BatchNormalization()(branch1)\n",
    "    branch1=ReLU()(branch1)\n",
    "    branch1=Dropout(0.5)(branch1)\n",
    "    branch1=GlobalMaxPooling1D()(branch1)\n",
    "    \n",
    "        # Branch 2\n",
    "    branch2 = Sequential()(emb)\n",
    "    branch2=LSTM(64, return_sequences=True)(branch2)\n",
    "    branch2=BatchNormalization()(branch2)\n",
    "    branch2=ReLU()(branch2)\n",
    "    branch2=Dropout(0.5)(branch2)\n",
    "    branch2=GlobalMaxPooling1D()(branch2)\n",
    "\n",
    "    concatenated = Concatenate()([branch1, branch2])\n",
    "\n",
    "    hid_layer = Dense(128, activation='relu')(concatenated)\n",
    "    dropout = Dropout(0.3)(hid_layer)\n",
    "    output_layer = Dense(10, activation='softmax')(dropout)\n",
    "    \n",
    "    classifier = models.Model(inputs=input, outputs=output_layer)\n",
    "    \n",
    "    classifier.compile(optimizer=optimizers.legacy.Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ViHSD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/ViHSD/train.csv\")\n",
    "test = pd.read_csv(\"../data/ViHSD/test.csv\")\n",
    "dev = pd.read_csv(\"../data/ViHSD/dev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label_id\n",
       "0    19886\n",
       "1     4162\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['label_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[\"free_text\"].copy()\n",
    "y_train = train[\"label_id\"].copy()\n",
    "\n",
    "X_test = test[\"free_text\"].copy()\n",
    "y_test = test[\"label_id\"].copy()\n",
    "\n",
    "X_dev = dev[\"free_text\"].copy()\n",
    "y_dev = dev[\"label_id\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.apply(preprocess)\n",
    "X_test = X_test.apply(preprocess)\n",
    "X_dev = X_dev.apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_feature, X_test_feature, X_dev_feature = tfidf_svd(X_train, X_test, X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_feature, X_test_feature, X_dev_feature = word2vec(X_train, X_test, X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_feature = np.array(tokenizer(list(X_train), max_length=300, padding='max_length', truncation=True)['input_ids'])\n",
    "X_test_feature = np.array(tokenizer(list(X_test), max_length=300, padding='max_length', truncation=True)['input_ids'])\n",
    "X_dev_feature = np.array(tokenizer(list(X_dev), max_length=300, padding='max_length', truncation=True)['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 23:07:53.422885: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-06 23:07:53.422989: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-06 23:07:53.423007: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-06 23:07:56.717127: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-06 23:07:56.717535: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-06 23:07:56.717547: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-06 23:07:56.717577: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-06 23:07:56.717611: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2898 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 23:08:04.905013: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-06-06 23:08:15.024919: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 36s 172ms/step - loss: 1.1610 - accuracy: 0.7233 - val_loss: 0.4391 - val_accuracy: 0.8099\n",
      "Epoch 2/100\n",
      "94/94 [==============================] - 15s 163ms/step - loss: 0.6327 - accuracy: 0.8326 - val_loss: 0.4582 - val_accuracy: 0.7915\n",
      "Epoch 3/100\n",
      "94/94 [==============================] - 15s 161ms/step - loss: 0.5513 - accuracy: 0.8542 - val_loss: 0.3864 - val_accuracy: 0.8368\n",
      "Epoch 4/100\n",
      "94/94 [==============================] - 15s 160ms/step - loss: 0.4973 - accuracy: 0.8644 - val_loss: 0.4184 - val_accuracy: 0.8155\n",
      "Epoch 5/100\n",
      "94/94 [==============================] - 15s 160ms/step - loss: 0.4501 - accuracy: 0.8792 - val_loss: 0.4509 - val_accuracy: 0.8073\n",
      "Epoch 6/100\n",
      "94/94 [==============================] - 15s 154ms/step - loss: 0.4054 - accuracy: 0.8891 - val_loss: 0.4427 - val_accuracy: 0.8245\n",
      "Epoch 7/100\n",
      "94/94 [==============================] - 15s 154ms/step - loss: 0.3709 - accuracy: 0.9003 - val_loss: 0.4783 - val_accuracy: 0.8125\n",
      "Epoch 8/100\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.3409 - accuracy: 0.9072Restoring model weights from the end of the best epoch: 3.\n",
      "94/94 [==============================] - 14s 152ms/step - loss: 0.3409 - accuracy: 0.9072 - val_loss: 0.5083 - val_accuracy: 0.8222\n",
      "Epoch 8: early stopping\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5, restore_best_weights=True,min_delta=0.001)\n",
    "class_weights = {0: 1.0, 1: 4.0}\n",
    "\n",
    "model_hsd = create_lstm_model()\n",
    "history = model_hsd.fit(X_train_feature, y_train, \n",
    "                    validation_data=(X_dev_feature, y_dev), \n",
    "                    batch_size=256, epochs=100, verbose=True,\n",
    "                    callbacks=[early_stopping],\n",
    "                    class_weight=class_weights\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hsd.save('../models/model_lstm_hsd_2_phobert.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209/209 [==============================] - 10s 42ms/step\n",
      "Accuracy: 0.8374251497005988\n",
      "Precision: 0.5146871008939975\n",
      "Recall: 0.7120141342756183\n",
      "F1-micro: 0.8374251497005988\n",
      "F1-macro: 0.7478112760322263\n"
     ]
    }
   ],
   "source": [
    "model_eva = create_lstm_model()\n",
    "# model_eva.load_weights('../models/model_lstm_hsd_2_w2v.h5')\n",
    "model_eva= model_hsd\n",
    "\n",
    "y_pred = model_eva.predict(X_test_feature)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "evaluation(y_pred_classes, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 - micro: 0.8374251497005988\n",
      "F1 - macro: 0.7478112760322263\n",
      "Accuracy: 0.8374251497005988\n",
      "Precision - macro: 0.7254702614364394\n",
      "Recall - macro: 0.7875139164528777\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "# T√≠nh to√°n c√°c ch·ªâ s·ªë ƒë√°nh gi√°\n",
    "f1_micro = f1_score(y_test, y_pred_classes, average='micro')\n",
    "print(\"F1 - micro: \" + str(f1_micro))\n",
    "\n",
    "f1_macro = f1_score(y_test, y_pred_classes, average='macro')\n",
    "print(\"F1 - macro: \" + str(f1_macro))\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "print(\"Accuracy: \" + str(accuracy))\n",
    "\n",
    "precision_macro = precision_score(y_test, y_pred_classes, average='macro')\n",
    "print(\"Precision - macro: \" + str(precision_macro))\n",
    "\n",
    "recall_macro = recall_score(y_test, y_pred_classes, average='macro')\n",
    "print(\"Recall - macro: \" + str(recall_macro))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 283ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text =\"M·ªôt  ph√∫t  b√≥c  ƒë·ªìng  b·∫±ng  c·∫£  ƒë·ªùi  b√≥c  c·ª©c  üòÇ\"\n",
    "\n",
    "text = preprocess(text)\n",
    "\n",
    "sequences = tokenizer(text, max_length=300, padding='max_length', truncation=True)['input_ids']\n",
    "\n",
    "y_ =model_hsd.predict([sequences])\n",
    "np.argmax(y_, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L·∫•y c√°i n√†y\n",
    "# '../models/model_lstm_hsd_2_w2v.h5'\n",
    "\n",
    "# 2\n",
    "\n",
    "# w2v\n",
    "\n",
    "# Accuracy: 0.8670658682634731\n",
    "# Precision: 0.5893118594436311\n",
    "# Recall: 0.7111307420494699\n",
    "# F1-micro: 0.8670658682634731\n",
    "# F1-macro: 0.7813813562357896\n",
    "\n",
    "# F1 - micro: 0.8670658682634731\n",
    "# F1 - macro: 0.7813813562357896\n",
    "# Accuracy: 0.8670658682634731\n",
    "# Precision - macro: 0.7638881465076642\n",
    "# Recall - macro: 0.8050066111112526"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 5548, 1: 1132})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "element_counts = Counter(y_test)\n",
    "print(element_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 5314, 1: 1366})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "element_counts = Counter(y_pred_classes)\n",
    "print(element_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 3\n",
    "\n",
    "# Accuracy: 0.8591317365269461\n",
    "# Precision: 0.5920925747348119\n",
    "# Recall: 0.5424028268551236\n",
    "# F1-micro: 0.8591317365269461\n",
    "# F1-macro: 0.7410370473638934\n",
    "\n",
    "# w2v\n",
    "\n",
    "# Accuracy: 0.8565868263473054\n",
    "# Precision: 0.5663109756097561\n",
    "# Recall: 0.6563604240282686\n",
    "# F1-micro: 0.8565868263473054\n",
    "# F1-macro: 0.7601292776440715"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 1\n",
    "\n",
    "# tf-idf\n",
    "# Accuracy: 0.8494011976047904\n",
    "# Precision: 0.5474397590361446\n",
    "# Recall: 0.642226148409894\n",
    "# F1-micro: 0.8494011976047903\n",
    "# F1-macro: 0.7493816662937272"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CTSD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/ViCTSD/ViCTSD_train.csv\")\n",
    "test = pd.read_csv(\"../data/ViCTSD/ViCTSD_test.csv\")\n",
    "dev = pd.read_csv(\"../data/ViCTSD/ViCTSD_valid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # L·ªçc c√°c h√†ng c√≥ nh√£n 1\n",
    "# df_label_1 = train[train['Toxicity'] == 1]\n",
    "\n",
    "# # Nh√¢n ƒë√¥i c√°c h√†ng c√≥ nh√£n 1\n",
    "# df_label_1_doubled = pd.concat([df_label_1] * 3, ignore_index=True)\n",
    "\n",
    "# # K·∫øt h·ª£p v·ªõi DataFrame g·ªëc\n",
    "# train = pd.concat([train, df_label_1_doubled], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Toxicity\n",
       "0    6241\n",
       "1     759\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Toxicity\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # L·ªçc c√°c h√†ng c√≥ nh√£n 1\n",
    "# df_label_1 = dev[dev['Toxicity'] == 1]\n",
    "\n",
    "# # Nh√¢n ƒë√¥i c√°c h√†ng c√≥ nh√£n 1\n",
    "# df_label_1_doubled = pd.concat([df_label_1] * 3, ignore_index=True)\n",
    "\n",
    "# # K·∫øt h·ª£p v·ªõi DataFrame g·ªëc\n",
    "# dev = pd.concat([dev, df_label_1_doubled], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Toxicity\n",
       "0    1768\n",
       "1     232\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev[\"Toxicity\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[\"Comment\"].copy()\n",
    "y_train = train[\"Toxicity\"].copy()\n",
    "\n",
    "X_test = test[\"Comment\"].copy()\n",
    "y_test = test[\"Toxicity\"].copy()\n",
    "\n",
    "X_dev = dev[\"Comment\"].copy()\n",
    "y_dev = dev[\"Toxicity\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.apply(preprocess)\n",
    "X_test = X_test.apply(preprocess)\n",
    "X_dev = X_dev.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def augment_data(X, y, num_augmented_samples=10000):\n",
    "#     augmented_X = []\n",
    "#     augmented_y = []\n",
    "#     for _ in range(num_augmented_samples):\n",
    "#         idx = np.random.choice(len(X))\n",
    "#         sample = X[idx]\n",
    "#         label = y[idx]\n",
    "#         # Th·ª±c hi·ªán m·ªôt s·ªë ph√©p bi·∫øn ƒë·ªïi ng·∫´u nhi√™n, v√≠ d·ª• thay ƒë·ªïi m·ªôt s·ªë t·ª´ trong m·∫´u\n",
    "#         augmented_sample = sample.copy()\n",
    "#         change_idx = np.random.choice(len(sample), size=5, replace=False)\n",
    "#         augmented_sample[change_idx] = np.random.randint(1, 5000, size=5)\n",
    "#         augmented_X.append(augmented_sample)\n",
    "#         augmented_y.append(label)\n",
    "#     return np.array(augmented_X), np.array(augmented_y)\n",
    "# augmented_X, augmented_y = augment_data(X_train_feature, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_feature, X_test_feature, X_dev_feature = tfidf_svd(X_train, X_test, X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_feature, X_test_feature, X_dev_feature = word2vec(X_train, X_test, X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_feature = np.array(tokenizer(list(X_train), max_length=300, padding='max_length', truncation=True)['input_ids'])\n",
    "X_test_feature = np.array(tokenizer(list(X_test), max_length=300, padding='max_length', truncation=True)['input_ids'])\n",
    "X_dev_feature = np.array(tokenizer(list(X_dev), max_length=300, padding='max_length', truncation=True)['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_cate = to_categorical(y_train)\n",
    "# y_test_cate = to_categorical(y_test)\n",
    "# y_dev_cate = to_categorical(y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "28/28 [==============================] - 11s 222ms/step - loss: 1.8395 - accuracy: 0.6690 - val_loss: 0.5108 - val_accuracy: 0.8840\n",
      "Epoch 2/100\n",
      "28/28 [==============================] - 5s 171ms/step - loss: 1.0796 - accuracy: 0.7394 - val_loss: 0.5114 - val_accuracy: 0.8840\n",
      "Epoch 3/100\n",
      "28/28 [==============================] - 5s 175ms/step - loss: 0.9194 - accuracy: 0.7903 - val_loss: 0.4011 - val_accuracy: 0.8670\n",
      "Epoch 4/100\n",
      "28/28 [==============================] - 5s 165ms/step - loss: 0.6467 - accuracy: 0.8526 - val_loss: 0.4754 - val_accuracy: 0.7730\n",
      "Epoch 5/100\n",
      "28/28 [==============================] - 5s 167ms/step - loss: 0.4331 - accuracy: 0.8869 - val_loss: 0.4898 - val_accuracy: 0.8050\n",
      "Epoch 6/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.3011 - accuracy: 0.9197Restoring model weights from the end of the best epoch: 3.\n",
      "28/28 [==============================] - 5s 163ms/step - loss: 0.3011 - accuracy: 0.9197 - val_loss: 0.5334 - val_accuracy: 0.8095\n",
      "Epoch 6: early stopping\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3, restore_best_weights=True,min_delta=0.001)\n",
    "class_weights = {0: 1.0, 1: 5.0}\n",
    "model_ctsd = create_lstm_model()\n",
    "with tf.device('/gpu:0'):\n",
    "    history = model_ctsd.fit(X_train_feature, y_train, \n",
    "                        validation_data=(X_dev_feature, y_dev),\n",
    "                        epochs=100, batch_size=256, verbose=True,\n",
    "                        callbacks=[early_stopping],\n",
    "                        class_weight=class_weights\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ctsd.save('../models/model_lstm_ctsd_2_phobert.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = model_eva.predict([X_test_feature,X_test_feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 2s 44ms/step\n",
      "Accuracy: 0.88\n",
      "Precision: 0.43243243243243246\n",
      "Recall: 0.2909090909090909\n",
      "F1-micro: 0.88\n",
      "F1-macro: 0.6408733959011683\n"
     ]
    }
   ],
   "source": [
    "model_eva = create_lstm_model()\n",
    "# model_eva.load_weights('../models/model_lstm_ctsd_2_w2v.h5')\n",
    "model_eva= model_ctsd\n",
    "\n",
    "y_pred = model_eva.predict(X_test_feature)\n",
    "y_pred_classes = np.argmax(y_pred, axis=-1)\n",
    "\n",
    "evaluation(y_pred_classes, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 - micro: 0.88\n",
      "F1 - macro: 0.6408733959011683\n",
      "Accuracy: 0.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision - macro: 0.6740995855466698\n",
      "Recall - macro: 0.621859039836568\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "# T√≠nh to√°n c√°c ch·ªâ s·ªë ƒë√°nh gi√°\n",
    "f1_micro = f1_score(y_test, y_pred_classes, average='micro')\n",
    "print(\"F1 - micro: \" + str(f1_micro))\n",
    "\n",
    "f1_macro = f1_score(y_test, y_pred_classes, average='macro')\n",
    "print(\"F1 - macro: \" + str(f1_macro))\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "print(\"Accuracy: \" + str(accuracy))\n",
    "\n",
    "precision_macro = precision_score(y_test, y_pred_classes, average='macro')\n",
    "print(\"Precision - macro: \" + str(precision_macro))\n",
    "\n",
    "recall_macro = recall_score(y_test, y_pred_classes, average='macro')\n",
    "print(\"Recall - macro: \" + str(recall_macro))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 890, 1: 110})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "element_counts = Counter(y_test)\n",
    "print(element_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 926, 1: 74})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "element_counts = Counter(y_pred_classes)\n",
    "print(element_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L·∫•y c√°i n√†y\n",
    "# '../models/model_lstm_ctsd_2_w2v.h5'\n",
    "\n",
    "# # 2\n",
    "\n",
    "# Accuracy: 0.869\n",
    "# Precision: 0.4198473282442748\n",
    "# Recall: 0.5\n",
    "# F1-micro: 0.869\n",
    "# F1-macro: 0.6909787011197894\n",
    "\n",
    "# F1 - micro: 0.869\n",
    "# F1 - macro: 0.6909787011197894\n",
    "# Accuracy: 0.869\n",
    "# Precision - macro: 0.6782780945018843\n",
    "# Recall - macro: 0.7073033707865168\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "\n",
    "# Accuracy: 0.879\n",
    "# Precision: 0.39622641509433965\n",
    "# Recall: 0.19090909090909092\n",
    "# F1-micro: 0.879\n",
    "# F1-macro: 0.5959002240916939\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 3\n",
    "\n",
    "# Accuracy: 0.886\n",
    "# Precision: 0.47297297297297297\n",
    "# Recall: 0.3181818181818182\n",
    "# F1-micro: 0.886\n",
    "# F1-macro: 0.65882972610611"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
