{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Input, Reshape, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# project_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)), '..')\n",
    "project_dir = os.path.abspath(os.path.join(os.path.dirname(os.path.realpath('__file__')), '..'))\n",
    "data_dir = os.path.join(project_dir, 'preprocessing')\n",
    "sys.path.append(data_dir)\n",
    "\n",
    "from preprocess import preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(y_pred, y_test):\n",
    "    # Tính toán các chỉ số đánh giá\n",
    "    accuracy  = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1_micro = f1_score(y_test, y_pred, average='micro')\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1-micro: {f1_micro}\")\n",
    "    print(f\"F1-macro: {f1_macro}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_svd(X_train, X_test, X_dev):\n",
    "    tfidf_vect = TfidfVectorizer(analyzer='word', max_features=30000)\n",
    "    tfidf_vect.fit(X_train) # learn vocabulary and idf from training set\n",
    "    X_train_tfidf =  tfidf_vect.transform(X_train)\n",
    "    X_test_tfidf =  tfidf_vect.transform(X_test)\n",
    "    X_dev_tfidf =  tfidf_vect.transform(X_dev)\n",
    "\n",
    "    svd = TruncatedSVD(n_components=300, random_state=42)\n",
    "    svd.fit(X_train_tfidf)\n",
    "\n",
    "    X_train_tfidf_svd = svd.transform(X_train_tfidf)\n",
    "    X_test_tfidf_svd = svd.transform(X_test_tfidf)\n",
    "    X_dev_tfidf_svd = svd.transform(X_dev_tfidf)\n",
    "\n",
    "    return X_train_tfidf_svd, X_test_tfidf_svd, X_dev_tfidf_svd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model():\n",
    "    input_layer = Input(shape=(300,))\n",
    "    \n",
    "    layer = Reshape((10, 30))(input_layer)\n",
    "    layer = LSTM(128, activation='relu')(layer)\n",
    "    layer = Dense(512, activation='relu')(layer)\n",
    "    layer = Dense(512, activation='relu')(layer)\n",
    "    layer = Dense(128, activation='relu')(layer)\n",
    "    \n",
    "    output_layer = Dense(10, activation='softmax')(layer)\n",
    "    \n",
    "    classifier = models.Model(input_layer, output_layer)\n",
    "    \n",
    "    classifier.compile(optimizer=optimizers.Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_lstm_model():\n",
    "#     input_layer = Input(shape=(300,))\n",
    "    \n",
    "#     layer = Reshape((10, 30))(input_layer)\n",
    "#     layer = LSTM(128, activation='relu')(layer)\n",
    "#     layer = Dropout(0.1)(layer)\n",
    "#     layer = Dense(512, activation='relu')(layer)\n",
    "#     layer = Dropout(0.1)(layer)\n",
    "#     layer = Dense(256, activation='relu')(layer)\n",
    "#     layer = Dropout(0.1)(layer)\n",
    "#     layer = Dense(128, activation='relu')(layer)\n",
    "#     layer = Dropout(0.1)(layer)\n",
    "#     layer = Dense(64, activation='relu')(layer)\n",
    "#     layer = Dropout(0.1)(layer)\n",
    "    \n",
    "    \n",
    "#     output_layer = Dense(10, activation='softmax')(layer)\n",
    "    \n",
    "#     classifier = models.Model(input_layer, output_layer)\n",
    "    \n",
    "#     classifier.compile(optimizer=optimizers.Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "#     return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ViHSD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/ViHSD/train.csv\")\n",
    "test = pd.read_csv(\"../data/ViHSD/test.csv\")\n",
    "dev = pd.read_csv(\"../data/ViHSD/dev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label_id\n",
       "0    19886\n",
       "1     4162\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['label_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[\"free_text\"].copy()\n",
    "y_train = train[\"label_id\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test[\"free_text\"].copy()\n",
    "y_test = test[\"label_id\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev = dev[\"free_text\"].copy()\n",
    "y_dev = dev[\"label_id\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.apply(preprocess)\n",
    "X_test = X_test.apply(preprocess)\n",
    "X_dev = X_dev.apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_feature, X_test_feature, X_dev_feature = tfidf_svd(X_train, X_test, X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "376/376 [==============================] - 8s 16ms/step - loss: 1.2378 - accuracy: 0.4505 - val_loss: 0.5866 - val_accuracy: 0.7717\n",
      "Epoch 2/100\n",
      "376/376 [==============================] - 5s 14ms/step - loss: 0.9282 - accuracy: 0.7634 - val_loss: 0.5911 - val_accuracy: 0.7017\n",
      "Epoch 3/100\n",
      "376/376 [==============================] - 5s 14ms/step - loss: 0.8482 - accuracy: 0.7902 - val_loss: 0.6009 - val_accuracy: 0.7620\n",
      "Epoch 4/100\n",
      "376/376 [==============================] - 5s 14ms/step - loss: 0.8141 - accuracy: 0.8031 - val_loss: 0.5069 - val_accuracy: 0.8005\n",
      "Epoch 5/100\n",
      "376/376 [==============================] - 5s 14ms/step - loss: 0.7954 - accuracy: 0.8075 - val_loss: 0.4005 - val_accuracy: 0.8406\n",
      "Epoch 6/100\n",
      "376/376 [==============================] - 6s 15ms/step - loss: 0.7780 - accuracy: 0.8074 - val_loss: 0.4794 - val_accuracy: 0.8121\n",
      "Epoch 7/100\n",
      "376/376 [==============================] - 6s 15ms/step - loss: 0.7557 - accuracy: 0.8130 - val_loss: 0.3834 - val_accuracy: 0.8488\n",
      "Epoch 8/100\n",
      "376/376 [==============================] - 5s 14ms/step - loss: 0.7455 - accuracy: 0.8120 - val_loss: 0.5173 - val_accuracy: 0.7631\n",
      "Epoch 9/100\n",
      "376/376 [==============================] - 5s 14ms/step - loss: 0.7333 - accuracy: 0.8108 - val_loss: 0.4826 - val_accuracy: 0.7912\n",
      "Epoch 10/100\n",
      "376/376 [==============================] - 5s 14ms/step - loss: 0.7267 - accuracy: 0.8099 - val_loss: 0.3811 - val_accuracy: 0.8484\n",
      "Epoch 11/100\n",
      "376/376 [==============================] - 5s 14ms/step - loss: 0.7081 - accuracy: 0.8179 - val_loss: 0.3856 - val_accuracy: 0.8488\n",
      "Epoch 12/100\n",
      "376/376 [==============================] - 5s 14ms/step - loss: 0.6915 - accuracy: 0.8165 - val_loss: 0.4314 - val_accuracy: 0.8241\n",
      "Epoch 13/100\n",
      "376/376 [==============================] - 6s 16ms/step - loss: 0.6866 - accuracy: 0.8245 - val_loss: 0.3902 - val_accuracy: 0.8406\n",
      "Epoch 14/100\n",
      "376/376 [==============================] - 5s 14ms/step - loss: 0.6846 - accuracy: 0.8214 - val_loss: 0.4126 - val_accuracy: 0.8368\n",
      "Epoch 15/100\n",
      "375/376 [============================>.] - ETA: 0s - loss: 0.6728 - accuracy: 0.8282Restoring model weights from the end of the best epoch: 10.\n",
      "376/376 [==============================] - 6s 15ms/step - loss: 0.6729 - accuracy: 0.8282 - val_loss: 0.4892 - val_accuracy: 0.8080\n",
      "Epoch 15: early stopping\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5, restore_best_weights=True,min_delta=0.001)\n",
    "class_weights = {0: 1.0, 1: 5.0}\n",
    "\n",
    "model_hsd = create_lstm_model()\n",
    "history = model_hsd.fit(X_train_feature, y_train, \n",
    "                    validation_data=(X_dev_feature, y_dev), \n",
    "                    batch_size=64, epochs=100, verbose=True,\n",
    "                    callbacks=[early_stopping],\n",
    "                    class_weight=class_weights\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model_hsd.save('../models/model_lstm_hsd.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209/209 [==============================] - 2s 8ms/step\n",
      "Accuracy: 0.8494011976047904\n",
      "Precision: 0.5474397590361446\n",
      "Recall: 0.642226148409894\n",
      "F1-micro: 0.8494011976047903\n",
      "F1-macro: 0.7493816662937272\n"
     ]
    }
   ],
   "source": [
    "model_eva = create_lstm_model()\n",
    "# model_eva.load_weights('../models/model_lstm_ctsd.h5')\n",
    "model_eva= model_hsd\n",
    "\n",
    "y_pred = model_eva.predict(X_test_feature)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "evaluation(y_pred_classes, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy: 0.8494011976047904\n",
    "# Precision: 0.5474397590361446\n",
    "# Recall: 0.642226148409894\n",
    "# F1-micro: 0.8494011976047903\n",
    "# F1-macro: 0.7493816662937272"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CTSD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/ViCTSD/ViCTSD_train.csv\")\n",
    "test = pd.read_csv(\"../data/ViCTSD/ViCTSD_test.csv\")\n",
    "dev = pd.read_csv(\"../data/ViCTSD/ViCTSD_valid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lọc các hàng có nhãn 1\n",
    "# df_label_1 = train[train['Toxicity'] == 1]\n",
    "\n",
    "# # Nhân đôi các hàng có nhãn 1\n",
    "# df_label_1_doubled = pd.concat([df_label_1] * 3, ignore_index=True)\n",
    "\n",
    "# # Kết hợp với DataFrame gốc\n",
    "# train = pd.concat([train, df_label_1_doubled], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Toxicity\n",
       "0    6241\n",
       "1     759\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Toxicity\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[\"Comment\"].copy()\n",
    "y_train = train[\"Toxicity\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test[\"Comment\"].copy()\n",
    "y_test = test[\"Toxicity\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev = dev[\"Comment\"].copy()\n",
    "y_dev = dev[\"Toxicity\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.apply(preprocess)\n",
    "X_test = X_test.apply(preprocess)\n",
    "X_dev = X_dev.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def augment_data(X, y, num_augmented_samples=10000):\n",
    "#     augmented_X = []\n",
    "#     augmented_y = []\n",
    "#     for _ in range(num_augmented_samples):\n",
    "#         idx = np.random.choice(len(X))\n",
    "#         sample = X[idx]\n",
    "#         label = y[idx]\n",
    "#         # Thực hiện một số phép biến đổi ngẫu nhiên, ví dụ thay đổi một số từ trong mẫu\n",
    "#         augmented_sample = sample.copy()\n",
    "#         change_idx = np.random.choice(len(sample), size=5, replace=False)\n",
    "#         augmented_sample[change_idx] = np.random.randint(1, 5000, size=5)\n",
    "#         augmented_X.append(augmented_sample)\n",
    "#         augmented_y.append(label)\n",
    "#     return np.array(augmented_X), np.array(augmented_y)\n",
    "# augmented_X, augmented_y = augment_data(X_train_feature, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_feature, X_test_feature, X_dev_feature = tfidf_svd(X_train, X_test, X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "219/219 [==============================] - 4s 12ms/step - loss: 1.0819 - accuracy: 0.8603 - val_loss: 0.5825 - val_accuracy: 0.8840\n",
      "Epoch 2/100\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.9565 - accuracy: 0.8869 - val_loss: 0.5276 - val_accuracy: 0.8840\n",
      "Epoch 3/100\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.9267 - accuracy: 0.8417 - val_loss: 0.3892 - val_accuracy: 0.8700\n",
      "Epoch 4/100\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.8808 - accuracy: 0.7703 - val_loss: 0.3870 - val_accuracy: 0.8685\n",
      "Epoch 5/100\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.8416 - accuracy: 0.7811 - val_loss: 0.5348 - val_accuracy: 0.7270\n",
      "Epoch 6/100\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.8036 - accuracy: 0.7954 - val_loss: 0.5082 - val_accuracy: 0.7905\n",
      "Epoch 7/100\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.7721 - accuracy: 0.7967 - val_loss: 0.4533 - val_accuracy: 0.7955\n",
      "Epoch 8/100\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.7333 - accuracy: 0.8110 - val_loss: 0.4699 - val_accuracy: 0.7950\n",
      "Epoch 9/100\n",
      "219/219 [==============================] - 3s 16ms/step - loss: 0.7154 - accuracy: 0.8103 - val_loss: 0.3379 - val_accuracy: 0.8830\n",
      "Epoch 10/100\n",
      "219/219 [==============================] - 5s 23ms/step - loss: 0.7197 - accuracy: 0.8131 - val_loss: 0.4596 - val_accuracy: 0.8135\n",
      "Epoch 11/100\n",
      "219/219 [==============================] - 5s 24ms/step - loss: 0.6815 - accuracy: 0.8161 - val_loss: 0.6023 - val_accuracy: 0.5830\n",
      "Epoch 12/100\n",
      "219/219 [==============================] - 5s 25ms/step - loss: 0.6458 - accuracy: 0.8327 - val_loss: 0.4709 - val_accuracy: 0.7840\n",
      "Epoch 13/100\n",
      "219/219 [==============================] - 3s 12ms/step - loss: 0.6309 - accuracy: 0.8409 - val_loss: 0.4936 - val_accuracy: 0.7670\n",
      "Epoch 14/100\n",
      "219/219 [==============================] - ETA: 0s - loss: 0.6171 - accuracy: 0.8223Restoring model weights from the end of the best epoch: 9.\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.6171 - accuracy: 0.8223 - val_loss: 0.4088 - val_accuracy: 0.8250\n",
      "Epoch 14: early stopping\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5, restore_best_weights=True,min_delta=0.001)\n",
    "class_weights = {0: 1.0, 1: 5.0}\n",
    "model_ctsd = create_lstm_model()\n",
    "history = model_ctsd.fit(X_train_feature, y_train, \n",
    "                    validation_data=(X_dev_feature, y_dev),\n",
    "                    epochs=100, batch_size=32, verbose=True,\n",
    "                    callbacks=[early_stopping],\n",
    "                    class_weight=class_weights\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model_ctsd.save('../models/model_lstm_ctsd.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Accuracy: 0.879\n",
      "Precision: 0.39622641509433965\n",
      "Recall: 0.19090909090909092\n",
      "F1-micro: 0.879\n",
      "F1-macro: 0.5959002240916939\n"
     ]
    }
   ],
   "source": [
    "model_eva = create_lstm_model()\n",
    "model_eva.load_weights('../models/model_lstm_ctsd.h5')\n",
    "# model_eva= model_ctsd\n",
    "\n",
    "y_pred = model_eva.predict(X_test_feature)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "evaluation(model_eva, X_test_feature, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 947, 1: 53})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "element_counts = Counter(y_pred_classes)\n",
    "print(element_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy: 0.879\n",
    "# Precision: 0.39622641509433965\n",
    "# Recall: 0.19090909090909092\n",
    "# F1-micro: 0.879\n",
    "# F1-macro: 0.5959002240916939"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
